\section{Related work}

Our approach builds on the research done in the fields of Virtual Reality (VR), haptics, neuroscience and cognitive psychology. 

\subsection{Assessing immersion/presence in VR}

One of the most commonly used questionnaires for evaluating how a user experiences presence in a virtual environment is the Igroup Presence Questionnaire (IPQ) ~\cite{ipq_paper}. The questionnaire spans over 14 questions on four domains: general presence, spatial presence, involvement and experienced realism. Its broad application scope makes it a widely adopted metric used by~\cite{ipq_CHI_2018,ipq_CHI_2016}, just to cite a few. The authors of the IPQ state that they interpret sense of presence as an individual experience and therefore a matter of subjective rating scales. However, researchers have critiqued this approach precisely due to its subjectivity. For instance, Slater elaborated a critique on these metrics in~\cite{slater_measuring_1999} and as Garvia-Valle et al. put it "presence is a subjective parameter, and that is why the results depend on the participant opinion"; they even add "their answers depend on their level of expertise"~\cite{garcia-valle_evaluation_2018}. 

\subsection{Assessing haptic immersion}
There are many ways to evaluate a haptic device, one of the more established is the Just-noticeable difference (JND)~\cite{stern_just_2010}. This study design allows researchers to measure the perceptual threshold by forcing the user to consider whether two haptic events are dissimilar. While methods such as the JND are very popular (e.g.,~\cite{pongrac_vibrotactile_2008,allin_measuring_2002}), they target the user's perceptual apparatus and are not a measure of haptic immersion in a VR environment. Thus, many researchers rely the presence questionnaire to assess haptic immersion. 

The IPQ has been used by many researchers seeking to better understand their haptic devices. Just to exemplify a few: a vibrotactile cane for blind VR users~\cite{cane_CHI_2018}; a multi-haptics interface that uses a combination of vibration, wind, water spray and heat~\cite{reckter_tech-note_2009}; a wind feedback device based on head-mounted fans~\cite{lehmann_poster_2009}; vibration feedback for roller-coaster experiences~\cite{alqassab_impact_2016}; an exoskeleton that provides force-feedback~\cite{calvo_body-grounded_2017}; and so forth. 

In certain cases, the researchers selected a few questions of the standard IPQ, but felt the need to append haptic specific questions. For example, when Calvo et al. evaluate their aforementioned exoskeleton device in a bow and arrow VR simulator~\cite{calvo_body-grounded_2017}, they appended two haptic specific questions to the IPQ, so they could evaluate the sensation of pulling on the bowstring. Moreover, there seems to be an abundance of recent research in VR haptics that does not use the IPQ at all and, instead, authors craft their own questions directly targeted at their devices (e.g.,~\cite{garcia-valle_evaluation_2018,amphibian_CHI,lopes_impacto:_2015}). These examples propelled us to explore a complementary or perhaps even alternative method to detect conflicts in the user's sense of haptic immersion.

\subsection{The impact of realism in brain responses}

The idea to analyze the user's brain response as an indicator of the user's state has become increasingly popular at the intersection of neuroscience and HCI~\cite{Yuksel:2016:LPB:2858036.2858388, berka_eeg_2007,tan_enhancing_2010}. 

For instance, Zander et al. revealed that, as users observed a cursor moving towards a target on a screen, any deviation from the user's expectation about the cursor path was reflected in the amplitude of the prediction error negativity (PEN)~\cite{zander_neuroadaptive_2016}. Similarly, Holroyd et al. found out that participants exhibit a negative potential around 200ms after seeing a visual stimulus that fell outside their expectations~\cite{holroyd_feedback_2008}. Along the same lines, Coles et al. found that the negative evoked potentials are indeed sensitive to the processing of incoming stimulus~\cite{coles_why_2001}. These studies utilized precisely the same component of the ERP we propose for detecting visuo-haptic conflicts. 

More recently, Singh et al. demonstrated that, in an object selection task in VR, the PEN component of the ERP was more pronounced for incorrect feedback when the user's hand was represented by a realistic hand avatar as compared to unrealistic representations~\cite{singh_visual_2018}. Furthermore, they found the PEN amplitude correlated with the level of realism of the avatar hand, as suggested by the uncanny valley theory~\cite{mori1970uncanny}. Also, Padrao et al. have shown, in a VR simulation, a differentiation of both the ERP amplitudes and latency between self-generated and observed avatar behavioral errors in a reaction time task prone designed to provoke errors in the trials~\cite{Padrao2016}. 

Moreover, Gonzalez-Franco and Lanier recently argued that it is precisely the sensory prediction model that enables all illusions in VR to take place~\cite{10.3389/fpsyg.2017.01125}; thus, it is a key neural mechanism for understanding immersion. Taken together, these results show a link between the prediction error signal and the level of immersion, suggesting that the increased immersive experience of the user is reflected in an increased sensitivity to deviations from the expected changes in the VR environment during the interaction. 

%Behavioral responses as well as amplitude modulations of the PEN were sensitive for unexpected as compared to expected feedback reflecting object selection. Furthermore, the PEN amplitude correlated with the level of realism of the avatar hand, as uncanny valley theory suggests [ref]. Taken together, these results show a covariation between the prediction error signal and the level of immersion and suggest that the increased immersive experience of the user is reflected in an increased sensitivity to deviations from the expected changes in the VR environment during the interaction.

%. Most of these studies, however, use sparse sensory stimulation that require minimal motor responses in movement-restricted participants. This is not how humans interact with their natural environment which often dynamically changes and requires complex behaviors providing multisensory experiences including vision, auditory, tactile, and proprioceptive information\missing{Gramann et al., 2011 IntJNeurosci} To overcome the restrictions of established brain imaging modalities and to allow active behavior of participants including mulit-modal sensory integration, the \textbf{Mobile Brain/Body Imaging (MoBI)} approach was developed\missing{Makeig et al., 2009; 2014}. MoBI studies demonstrate changes in brain dynamics when participants interact with dynamically changing environments \missing{Jungnickel et al., 2017} or when allowed to integrate multi-modal information stemming from the visual, vestibular, and proprioceptive senses as compared to established only visual information\missing{Gramann et al, 2018 bioRxiv.} These studies thus provide the methodological approach to investigate and reveal differences in human  brain dynamics underlying natural cognition in realistic environments. 
